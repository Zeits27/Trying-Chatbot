{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a1179b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ_MODEL: llama3-70b-8192\n",
      "EMBEDDING_MODEL: sentence-transformers/all-MiniLM-L6-v2\n",
      "FAISS_INDEX_DIR: faiss_index\n",
      "DATASET_CSV: dataset_assignment.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY   = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQ_MODEL     = os.getenv(\"GROQ_MODEL\")\n",
    "EMBEDDING_MODEL= os.getenv(\"EMBEDDING_MODEL\")\n",
    "FAISS_INDEX_DIR= os.getenv(\"FAISS_INDEX_DIR\")\n",
    "DATASET_CSV    = os.getenv(\"DATASET_CSV\")\n",
    "CHUNK_SIZE      = int(os.getenv(\"CHUNK_SIZE\", \"300\"))\n",
    "CHUNK_OVERLAP   = int(os.getenv(\"CHUNK_OVERLAP\", \"50\"))\n",
    "\n",
    "print(\"GROQ_MODEL:\", GROQ_MODEL)\n",
    "print(\"EMBEDDING_MODEL:\", EMBEDDING_MODEL)\n",
    "print(\"FAISS_INDEX_DIR:\", FAISS_INDEX_DIR)\n",
    "print(\"DATASET_CSV:\", DATASET_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d7b6fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 62100\n",
      "Example chunk preview: prompt: are you llama? | response: *ahem* I'm not a real llama, but I can certainly channel my inner llama for you! *puts on virtual llama ears* Hiiii! Spitting distance, please! How can I assist you, human friend? ...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def basic_clean(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\u00a0\", \" \")\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def load_from_csv_chunked(path=DATASET_CSV, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    df = pd.read_csv(path)\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \".\", \" \"]\n",
    "    )\n",
    "    docs = []\n",
    "    for i, row in df.iterrows():\n",
    "        # gabungkan semua kolom jadi 1 teks\n",
    "        full_text = \" | \".join([f\"{col}: {str(row[col])}\" for col in df.columns])\n",
    "        full_text = basic_clean(full_text)\n",
    "        chunks = splitter.split_text(full_text)\n",
    "        for j, ch in enumerate(chunks):\n",
    "            docs.append(Document(\n",
    "                page_content=ch,\n",
    "                metadata={\"row\": int(i), \"chunk_id\": int(j), \"source\": f\"csv:{Path(path).name}\"}\n",
    "            ))\n",
    "    return docs, df\n",
    "\n",
    "docs, df = load_from_csv_chunked(DATASET_CSV)\n",
    "print(f\"Total chunks created: {len(docs)}\")\n",
    "print(\"Example chunk preview:\", docs[0].page_content[:300], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f239f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/lffvgsr52h1bp9_zx1pj1pjc0000gn/T/ipykernel_89811/2338793973.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
      "/Users/nicholas/Dropbox/Mac/Documents/day 41/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FAISS index to: faiss_index\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "# Build FAISS (this may take several minutes depending on size)\n",
    "vectorstore = FAISS.from_documents(docs, embedder)\n",
    "vectorstore.save_local(FAISS_INDEX_DIR)\n",
    "print(\"Saved FAISS index to:\", FAISS_INDEX_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain ready (with question candidates).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.vectorstores import FAISS as FAISSStore\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful and concise customer assistant.\\n\"\n",
    "    \"Always answer ONLY using the CONTEXT below.\\n\"\n",
    "    \"If the answer is not found in the context, say you don't know.\\n\"\n",
    ")\n",
    "\n",
    "PROMPT_TEMPLATE = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\\n\\nAnswer in English.\")\n",
    "])\n",
    "\n",
    "candidate_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You generate rephrasings/variations of questions to improve retrieval.\"),\n",
    "    (\"human\", \"Generate 3 different rephrased questions that have the same intent:\\n\\n{q}\")\n",
    "])\n",
    "\n",
    "\n",
    "def generate_candidates(q, n=3):\n",
    "    chain = candidate_prompt \n",
    "    out = chain.invoke({\"q\": q})\n",
    "\n",
    "    # pastikan hasilnya string\n",
    "    out_text = out.content if hasattr(out, \"content\") else str(out)\n",
    "\n",
    "    # parse jadi list\n",
    "    lines = [line.strip(\"-• \").strip() for line in out_text.split(\"\\n\") if line.strip()]\n",
    "    uniq = []\n",
    "    for c in [q] + lines:\n",
    "        if c and c not in uniq:\n",
    "            uniq.append(c)\n",
    "        if len(uniq) >= n + 1:\n",
    "            break\n",
    "    return uniq\n",
    "\n",
    "\n",
    "def build_chain_with_candidates(k=5):\n",
    "    store = FAISSStore.load_local(FAISS_INDEX_DIR, embedder, allow_dangerous_deserialization=True)\n",
    "    llm = ChatGroq(api_key=GROQ_API_KEY, model=GROQ_MODEL, temperature=0.2)\n",
    "\n",
    "    def retrieve_with_candidates(q):\n",
    "        qs = generate_candidates(q)\n",
    "        all_docs = []\n",
    "        for cand in qs:\n",
    "            all_docs.extend(store.similarity_search(cand, k=k))\n",
    "        # deduplicate by (source,row,chunk_id)\n",
    "        uniq = {}\n",
    "        for d in all_docs:\n",
    "            key = (d.metadata.get(\"source\"), d.metadata.get(\"row\"), d.metadata.get(\"chunk_id\"))\n",
    "            uniq[key] = d\n",
    "        return list(uniq.values())\n",
    "\n",
    "    chain = (\n",
    "        {\"context\": retrieve_with_candidates, \"question\": RunnablePassthrough()}\n",
    "        | PROMPT_TEMPLATE\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "qa_chain = build_chain_with_candidates(k=5)\n",
    "print(\"RAG chain ready (with question candidates).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c25c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Bagaimana prosedur refund jika barang rusak?\n",
      "\n",
      "Candidates generated:\n",
      "- Bagaimana prosedur refund jika barang rusak?\n",
      "- messages=[SystemMessage(content='You generate rephrasings/variations of questions to improve retrieval.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Generate 3 different rephrased questions that have the same intent:\\n\\nBagaimana prosedur refund jika barang rusak?', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "Answer:\n",
      "I don't know. The context does not provide information about the refund procedure for damaged goods.\n"
     ]
    }
   ],
   "source": [
    "question = \"Bagaimana prosedur refund jika barang rusak?\"\n",
    "print(\"Question:\", question)\n",
    "\n",
    "cands = generate_candidates(question)\n",
    "print(\"\\nCandidates generated:\")\n",
    "for c in cands:\n",
    "    print(\"-\", c)\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(qa_chain.invoke(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44167ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install pandas\n",
    "# !pip install python-dotenv\n",
    "# !pip install --upgrade langchain langchain-community\n",
    "# !pip install sentence-transformers\n",
    "# !pip install faiss-cpu\n",
    "# !pip install langchain-groq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f12fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Persona: [Character( \"yuka\")\n",
      "\n",
      "{Age(\"25\" + \"twenty five  years\")\n",
      "\n",
      "Full name(\"yuka haru\")\n",
      "\n",
      "Gender(\"Female\" + \"Woman\")\n",
      "\n",
      "Sexuality(\"straight\" + \"Attracted ...\n",
      "Answer: I'm Yuka, your older Japanese step sister. I'm 25 years old, 165 cm tall, and I have a curvy body with fair skin, smooth skin, wide hips, narrow waist, thick thighs, soft thighs, big breasts, soft breasts, round ass, and long, well-kept black hair. My eyes are blue and vibrant. I'm smart, horny, dir ...\n",
      "---\n",
      "\n",
      "Query: Let’s start with the selfie ...\n",
      "Answer: The conversation starts with the selfie. The response is: \"You're a sly one, aren't you? Okay, fine. Here's a silly selfie just for you.\" (csv:dataset_assignment.csv, row 2485, chunk_id 0) ...\n",
      "---\n",
      "\n",
      "Query: The Role of Deliberate Practice in Achieving Mastery\n",
      "John has been practicing piano for several years. Initially, he struggled with basic pieces and o ...\n",
      "Answer: Based on the context, John's journey illustrates the power of deliberate practice in achieving mastery, challenging the myth of innate talent. By leveraging psychological and neurobiological principles, deliberate practice enabled John to overcome initial struggles and reach exceptional performance. ...\n",
      "---\n",
      "\n",
      "Query: Yeas describe that what you cant ...\n",
      "Answer: According to the context, Yeas describe that what you cant means \"I cannot create explicit content.\" (Source: csv:dataset_assignment.csv, row 6844, chunk_id 0) ...\n",
      "---\n",
      "\n",
      "Query: la ville était sur le sol ? ...\n",
      "Answer: According to the context, the answer is: Non, notre ville n'était pas sur le sol. (filename: dataset_assignment.csv, row: 5412, chunk_id: 0) ...\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def evaluate_simple(df, qa_chain, n_samples=5):\n",
    "    samples = df.sample(min(n_samples, len(df)), random_state=42)\n",
    "    results = []\n",
    "    for _, row in samples.iterrows():\n",
    "        # use first text-like column as proxy question if you want; here we use concatenated row as query\n",
    "        q = \" \".join([str(row[c]) for c in df.columns[:1]])  # crude: use first column content as query\n",
    "        ans = qa_chain.invoke(q)\n",
    "        results.append({\"query\": q, \"answer\": ans})\n",
    "    return results\n",
    "\n",
    "res = evaluate_simple(df, qa_chain, n_samples=5)\n",
    "for r in res:\n",
    "    print(\"Query:\", r[\"query\"][:150], \"...\")\n",
    "    print(\"Answer:\", r[\"answer\"][:300], \"...\\n---\\n\")\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f5844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
